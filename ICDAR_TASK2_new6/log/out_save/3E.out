nohup: ignoring input
/home/sjhbxs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
###########################################################
SparseTensor(indices=Tensor("BLSTM/Placeholder_2:0", shape=(?, ?), dtype=int64), values=Tensor("BLSTM/Placeholder_1:0", shape=(?,), dtype=int32), dense_shape=Tensor("BLSTM/Placeholder:0", shape=(?,), dtype=int64))
Tensor("transpose:0", shape=(?, ?, 102), dtype=float32)
Tensor("BLSTM/Placeholder_3:0", shape=(?,), dtype=int32)
loading train data, please wait--------------------- end= 
len is 0 or too long 1101121
len is 0 or too long 1067138
len is 0 or too long 1085492
len is 0 or too long 1080026
len is 0 or too long 1148610
len is 0 or too long 1161134
len is 0 or too long 1091637
len is 0 or too long 1234345
len is 0 or too long 1157057
len is 0 or too long 1078701
len is 0 or too long 1199831
len is 0 or too long 1124130
***************get image:  42606
loading validation data, please wait--------------------- end= 
***************get image:  35
2018-05-16 13:18:06.508456: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-16 13:18:06.508488: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-16 13:18:06.508495: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-05-16 13:18:06.508501: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-16 13:18:06.508506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-05-16 13:18:06.601619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-16 13:18:06.602037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:04.0
Total memory: 11.17GiB
Free memory: 11.08GiB
2018-05-16 13:18:06.602060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-05-16 13:18:06.602068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-05-16 13:18:06.602080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
restore is true
restore from the checkpoint/home/sjhbxs/checkout/ICDAR_task2/ICDAR_TASK2_new6/checkpoint/ocr-model-70800
=============================begin training=============================
2018-05-16 13:18:12.790467: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4578 get requests, put_count=3083 evicted_count=1000 eviction_rate=0.324359 and unsatisfied allocation rate=0.566841
2018-05-16 13:18:12.790521: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-05-16 13:18:23.174750: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 421 get requests, put_count=1434 evicted_count=1000 eviction_rate=0.69735 and unsatisfied allocation rate=0.0023753
2018-05-16 13:18:23.174828: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 160 to 176
2018-05-16 13:18:29.046902: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 652 get requests, put_count=1676 evicted_count=1000 eviction_rate=0.596659 and unsatisfied allocation rate=0.00153374
2018-05-16 13:18:36.294182: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5282 get requests, put_count=5131 evicted_count=2000 eviction_rate=0.389788 and unsatisfied allocation rate=0.414237
2018-05-16 13:18:36.294344: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 409 to 449
2018-05-16 13:18:43.624132: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5356 get requests, put_count=5473 evicted_count=2000 eviction_rate=0.36543 and unsatisfied allocation rate=0.362584
2018-05-16 13:18:43.624223: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-05-16 13:18:52.465646: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4889 get requests, put_count=4556 evicted_count=1000 eviction_rate=0.219491 and unsatisfied allocation rate=0.29413
2018-05-16 13:18:52.465738: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1158 to 1273
cur_epoch==== 0 cur_batch---- 99 g_step**** 70899 cost 3.2085292
seq   0: origin: [21, 18, 25, 18] decoded:[21, 18, 25, 18]
seq   1: origin: [69, 78, 84, 69, 82] decoded:[69, 78, 84, 69, 82, 82, 76]
seq   2: origin: [38, 57, 42, 53] decoded:[38, 57, 42, 53]
seq   3: origin: [51, 38, 34, 37, 42, 47, 40] decoded:[51, 38, 34, 37, 42, 47, 40]
seq   4: origin: [82, 85, 80, 65, 2] decoded:[85, 83, 76]
seq   5: origin: [53, 41, 34, 46, 38, 52] decoded:[53, 41, 34, 42, 46, 38, 52]
seq   6: origin: [52, 56, 38, 38, 53, 52] decoded:[52, 56, 38, 38, 45, 52]
seq   7: origin: [40, 51, 38, 38, 47, 56, 42, 36, 41] decoded:[44, 38, 47, 47, 42, 58]
seq   8: origin: [84, 79, 76, 84, 69, 67] decoded:[84, 79, 76, 73, 69, 67]
seq   9: origin: [37, 38, 52, 52, 38, 51, 53, 52] decoded:[37, 38, 52, 52, 38, 51, 53, 52]
seq  10: origin: [76, 73, 70, 69] decoded:[76, 73, 70, 69]
seq  11: origin: [39, 54, 43, 42] decoded:[39, 43, 43, 42]
seq  12: origin: [49, 72, 79, 84, 79] decoded:[80, 72, 79, 84, 79]
seq  13: origin: [51, 55, 36, 34] decoded:[51, 51, 45, 45, 44, 34, 42]
seq  14: origin: [36, 48, 56, 49, 38] decoded:[36, 48, 56, 49, 38]
seq  15: origin: [53, 51, 34, 55, 38, 45] decoded:[53, 51, 34, 55, 38, 45]
seq  16: origin: [22, 22, 14, 17, 18, 21, 22, 21] decoded:[20, 22, 14, 17, 18, 22, 20, 20]
seq  17: origin: [17, 26, 21, 14, 83, 71] decoded:[17, 26, 21, 14, 83, 71]
5/16 13:23:12  step===71000, Epoch 2/1000, accuracy = 0.429,avg_train_cost = 3.492, lastbatch_err = 0.257, time = 51.185,lr=0.00048989

cur_epoch==== 1 cur_batch---- 99 g_step**** 71065 cost 3.7036133
save checkpoint 71200
cur_epoch==== 2 cur_batch---- 99 g_step**** 71231 cost 3.8873935
cur_epoch==== 3 cur_batch---- 99 g_step**** 71397 cost 3.7968938
cur_epoch==== 4 cur_batch---- 99 g_step**** 71563 cost 3.4731011
save checkpoint 71600
cur_epoch==== 5 cur_batch---- 99 g_step**** 71729 cost 3.4112778
cur_epoch==== 6 cur_batch---- 99 g_step**** 71895 cost 3.932611
save checkpoint 72000
seq   0: origin: [21, 18, 25, 18] decoded:[21, 18, 25, 18]
seq   1: origin: [69, 78, 84, 69, 82] decoded:[69, 78, 84, 69, 82, 76]
seq   2: origin: [38, 57, 42, 53] decoded:[38, 57, 42, 53]
seq   3: origin: [51, 38, 34, 37, 42, 47, 40] decoded:[51, 38, 34, 37, 42, 47, 40]
seq   4: origin: [82, 85, 80, 65, 2] decoded:[85, 80, 76]
seq   5: origin: [53, 41, 34, 46, 38, 52] decoded:[53, 41, 34, 42, 46, 38, 52]
seq   6: origin: [52, 56, 38, 38, 53, 52] decoded:[52, 56, 38, 38, 53, 52]
seq   7: origin: [40, 51, 38, 38, 47, 56, 42, 36, 41] decoded:[44, 38, 47, 42, 58]
seq   8: origin: [84, 79, 76, 84, 69, 67] decoded:[84, 79, 76, 72, 69, 67]
seq   9: origin: [37, 38, 52, 52, 38, 51, 53, 52] decoded:[37, 38, 52, 52, 38, 51, 53, 52]
seq  10: origin: [76, 73, 70, 69] decoded:[76, 73, 70, 69]
seq  11: origin: [39, 54, 43, 42] decoded:[39, 43, 43, 42]
seq  12: origin: [49, 72, 79, 84, 79] decoded:[80, 72, 79, 84, 79]
seq  13: origin: [51, 55, 36, 34] decoded:[51, 45, 45, 44, 34, 42]
seq  14: origin: [36, 48, 56, 49, 38] decoded:[36, 48, 56, 49, 38]
seq  15: origin: [53, 51, 34, 55, 38, 45] decoded:[53, 51, 34, 55, 38, 45]
seq  16: origin: [22, 22, 14, 17, 18, 21, 22, 21] decoded:[22, 22, 14, 17, 18, 22, 20, 20]
seq  17: origin: [17, 26, 21, 14, 83, 71] decoded:[17, 26, 21, 14, 83, 71]
5/16 13:47:45  step===72000, Epoch 8/1000, accuracy = 0.457,avg_train_cost = 3.416, lastbatch_err = 0.230, time = 56.531,lr=0.00048499

cur_epoch==== 7 cur_batch---- 99 g_step**** 72061 cost 3.316296
cur_epoch==== 8 cur_batch---- 99 g_step**** 72227 cost 3.7239037
cur_epoch==== 9 cur_batch---- 99 g_step**** 72393 cost 3.7077384
save checkpoint 72400
cur_epoch==== 10 cur_batch---- 99 g_step**** 72559 cost 3.5986161
cur_epoch==== 11 cur_batch---- 99 g_step**** 72725 cost 3.6216345
save checkpoint 72800
cur_epoch==== 12 cur_batch---- 99 g_step**** 72891 cost 2.8533344
seq   0: origin: [21, 18, 25, 18] decoded:[21, 18, 25, 18]
seq   1: origin: [69, 78, 84, 69, 82] decoded:[69, 78, 84, 69, 82, 76]
seq   2: origin: [38, 57, 42, 53] decoded:[38, 57, 42, 53]
seq   3: origin: [51, 38, 34, 37, 42, 47, 40] decoded:[51, 38, 34, 37, 42, 47, 40]
seq   4: origin: [82, 85, 80, 65, 2] decoded:[85, 83, 76]
seq   5: origin: [53, 41, 34, 46, 38, 52] decoded:[53, 41, 34, 42, 46, 38, 52]
seq   6: origin: [52, 56, 38, 38, 53, 52] decoded:[52, 56, 38, 38, 45, 52]
seq   7: origin: [40, 51, 38, 38, 47, 56, 42, 36, 41] decoded:[44, 38, 47, 47, 42, 58]
seq   8: origin: [84, 79, 76, 84, 69, 67] decoded:[84, 79, 76, 72, 69, 67]
seq   9: origin: [37, 38, 52, 52, 38, 51, 53, 52] decoded:[37, 38, 52, 52, 38, 51, 53, 52]
seq  10: origin: [76, 73, 70, 69] decoded:[76, 73, 70, 69]
seq  11: origin: [39, 54, 43, 42] decoded:[39, 43, 43, 42]
seq  12: origin: [49, 72, 79, 84, 79] decoded:[80, 72, 79, 84, 79]
seq  13: origin: [51, 55, 36, 34] decoded:[51, 51, 45, 45, 44, 34, 42]
seq  14: origin: [36, 48, 56, 49, 38] decoded:[36, 48, 56, 49, 38]
seq  15: origin: [53, 51, 34, 55, 38, 45] decoded:[53, 51, 34, 55, 38, 45]
seq  16: origin: [22, 22, 14, 17, 18, 21, 22, 21] decoded:[22, 22, 14, 17, 18, 22, 20, 20]
seq  17: origin: [17, 26, 21, 14, 83, 71] decoded:[17, 26, 21, 14, 83, 71]
5/16 14:12:18  step===73000, Epoch 14/1000, accuracy = 0.429,avg_train_cost = 3.458, lastbatch_err = 0.244, time = 62.134,lr=0.00048014

cur_epoch==== 13 cur_batch---- 99 g_step**** 73057 cost 3.2607822
save checkpoint 73200
cur_epoch==== 14 cur_batch---- 99 g_step**** 73223 cost 3.5096664
cur_epoch==== 15 cur_batch---- 99 g_step**** 73389 cost 3.181322
cur_epoch==== 16 cur_batch---- 99 g_step**** 73555 cost 3.014385
save checkpoint 73600
cur_epoch==== 17 cur_batch---- 99 g_step**** 73721 cost 3.1423426
