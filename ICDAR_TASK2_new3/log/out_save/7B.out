nohup: ignoring input
/home/sjhbxs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
###########################################################
SparseTensor(indices=Tensor("BLSTM/Placeholder_2:0", shape=(?, ?), dtype=int64), values=Tensor("BLSTM/Placeholder_1:0", shape=(?,), dtype=int32), dense_shape=Tensor("BLSTM/Placeholder:0", shape=(?,), dtype=int64))
Tensor("transpose:0", shape=(?, ?, 102), dtype=float32)
Tensor("BLSTM/Placeholder_3:0", shape=(?,), dtype=int32)
loading train data, please wait--------------------- end= 
len is 0 1101121
len is 0 1067138
len is 0 1085492
len is 0 1080026
len is 0 1148610
len is 0 1161134
len is 0 1091637
len is 0 1234345
len is 0 1157057
len is 0 1078701
len is 0 1199831
len is 0 1124130
get image:  42606
loading validation data, please wait--------------------- end= 
get image:  422
2018-05-07 00:41:49.052512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-07 00:41:49.052544: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-07 00:41:49.052552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-05-07 00:41:49.052558: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-05-07 00:41:49.052564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-05-07 00:41:49.621020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-07 00:41:49.621344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:04.0
Total memory: 11.17GiB
Free memory: 11.09GiB
2018-05-07 00:41:49.621367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-05-07 00:41:49.621379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-05-07 00:41:49.621390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
restore is true
restore from the checkpoint/home/sjhbxs/checkout/ICDAR_task2/ICDAR_TASK2_new3/checkpoint/ocr-model-19600
=============================begin training=============================
2018-05-07 00:42:09.256780: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4561 get requests, put_count=3098 evicted_count=1000 eviction_rate=0.322789 and unsatisfied allocation rate=0.561938
2018-05-07 00:42:09.256839: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-05-07 00:42:20.771985: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 643 get requests, put_count=1658 evicted_count=1000 eviction_rate=0.603136 and unsatisfied allocation rate=0.00155521
2018-05-07 00:42:30.448444: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 642 get requests, put_count=1670 evicted_count=1000 eviction_rate=0.598802 and unsatisfied allocation rate=0
2018-05-07 00:42:46.152183: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5028 get requests, put_count=3984 evicted_count=1000 eviction_rate=0.251004 and unsatisfied allocation rate=0.415274
2018-05-07 00:42:46.152250: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 493 to 542
2018-05-07 00:43:04.992501: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4800 get requests, put_count=4279 evicted_count=1000 eviction_rate=0.233699 and unsatisfied allocation rate=0.335
2018-05-07 00:43:04.992553: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 958 to 1053
2018-05-07 00:43:36.747581: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10647 get requests, put_count=10799 evicted_count=1000 eviction_rate=0.0926012 and unsatisfied allocation rate=0.0971166
2018-05-07 00:43:36.747658: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2049 to 2253
batch 99 : time 3.086691379547119
save checkpoint 19700
save checkpoint 19800
batch 99 : time 3.1561455726623535
save checkpoint 19900
save checkpoint 20000
seq   0: origin: [37, 38, 45, 45] decoded:[37, 38, 45, 45]
seq   1: origin: [22, 24, 17, 19] decoded:[22, 24, 17, 19]
seq   2: origin: [18, 25, 26, 26] decoded:[18, 25, 26, 26]
seq   3: origin: [40, 45, 38, 47, 47, 37, 58, 47, 38] decoded:[40, 45, 38, 47, 47, 37, 58, 47, 38]
seq   4: origin: [51, 34, 49, 53, 48, 51, 52, 32] decoded:[51, 34, 49, 53, 48, 51, 52, 32]
seq   5: origin: [58, 79, 82, 75] decoded:[58, 79, 82, 75]
seq   6: origin: [74, 85, 78, 71, 76, 69] decoded:[74, 85, 78, 71, 76, 69]
seq   7: origin: [87, 79, 82, 75, 73, 78, 71] decoded:[87, 79, 82, 75, 73, 78, 71]
seq   8: origin: [84, 73, 82, 69, 68] decoded:[84, 73, 82, 69, 68]
seq   9: origin: [67, 65, 66, 65, 82, 69, 84] decoded:[67, 65, 66, 65, 82, 69, 84]
seq  10: origin: [53, 82, 65, 73, 78] decoded:[53, 82, 65, 73, 78]
seq  11: origin: [45, 42, 56, 38, 45, 45] decoded:[45, 42, 56, 38, 45, 45]
seq  12: origin: [53, 51, 42, 54, 46, 49, 41] decoded:[53, 51, 42, 54, 46, 49, 41]
seq  13: origin: [22, 18, 24, 14, 19, 18, 21, 18] decoded:[22, 18, 24, 14, 19, 18, 21, 18]
seq  14: origin: [36, 65, 83, 84, 82, 79, 76] decoded:[36, 65, 83, 84, 82, 79, 76]
seq  15: origin: [77, 69, 76, 69, 82] decoded:[77, 69, 76, 69, 82]
seq  16: origin: [67, 79, 70, 70, 69, 69] decoded:[67, 79, 70, 70, 69, 69]
seq  17: origin: [21, 24, 22, 17] decoded:[21, 24, 22, 17]
5/7 1:3:3  step===20000, Epoch 3/1000, accuracy = 0.979,avg_train_cost = 0.081, lastbatch_err = 0.004, time = 225.087,lr=0.00817907

batch 99 : time 3.276639223098755
save checkpoint 20100
batch 99 : time 3.0667550563812256
save checkpoint 20200
save checkpoint 20300
batch 99 : time 3.076852321624756
save checkpoint 20400
save checkpoint 20500
batch 99 : time 3.068368673324585
save checkpoint 20600
batch 99 : time 3.0514886379241943
save checkpoint 20700
save checkpoint 20800
batch 99 : time 3.0598902702331543
save checkpoint 20900
save checkpoint 21000
seq   0: origin: [37, 38, 45, 45] decoded:[37, 38, 45, 45]
seq   1: origin: [22, 24, 17, 19] decoded:[22, 24, 17, 19]
seq   2: origin: [18, 25, 26, 26] decoded:[18, 25, 26, 26]
seq   3: origin: [40, 45, 38, 47, 47, 37, 58, 47, 38] decoded:[40, 45, 38, 47, 47, 37, 58, 47, 38]
seq   4: origin: [51, 34, 49, 53, 48, 51, 52, 32] decoded:[51, 34, 49, 53, 48, 51, 52, 32]
seq   5: origin: [58, 79, 82, 75] decoded:[58, 79, 82, 75]
seq   6: origin: [74, 85, 78, 71, 76, 69] decoded:[74, 85, 78, 71, 76, 69]
seq   7: origin: [87, 79, 82, 75, 73, 78, 71] decoded:[87, 79, 82, 75, 73, 78, 71]
seq   8: origin: [84, 73, 82, 69, 68] decoded:[84, 73, 82, 69, 68]
seq   9: origin: [67, 65, 66, 65, 82, 69, 84] decoded:[67, 65, 66, 65, 82, 69, 84]
seq  10: origin: [53, 82, 65, 73, 78] decoded:[53, 82, 65, 73, 78]
seq  11: origin: [45, 42, 56, 38, 45, 45] decoded:[45, 42, 56, 38, 45, 45]
seq  12: origin: [53, 51, 42, 54, 46, 49, 41] decoded:[53, 51, 42, 54, 46, 49, 41]
seq  13: origin: [22, 18, 24, 14, 19, 18, 21, 18] decoded:[22, 18, 24, 14, 19, 18, 21, 18]
seq  14: origin: [36, 65, 83, 84, 82, 79, 76] decoded:[36, 65, 83, 84, 82, 79, 76]
seq  15: origin: [77, 69, 76, 69, 82] decoded:[77, 69, 76, 69, 82]
seq  16: origin: [67, 79, 70, 70, 69, 69] decoded:[67, 79, 70, 70, 69, 69]
seq  17: origin: [21, 24, 22, 17] decoded:[21, 24, 22, 17]
5/7 1:54:52  step===21000, Epoch 9/1000, accuracy = 0.988,avg_train_cost = 0.063, lastbatch_err = 0.002, time = 228.210,lr=0.00809728

batch 99 : time 3.152761459350586
save checkpoint 21100
batch 99 : time 3.273793935775757
save checkpoint 21200
save checkpoint 21300
batch 99 : time 3.06419038772583
save checkpoint 21400
save checkpoint 21500
batch 99 : time 2.0326826572418213
save checkpoint 21600
batch 99 : time 1.5497913360595703
save checkpoint 21700
save checkpoint 21800
