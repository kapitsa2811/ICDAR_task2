nohup: ignoring input
/home/sjhbxs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
###########################################################
SparseTensor(indices=Tensor("BLSTM/Placeholder_2:0", shape=(?, ?), dtype=int64), values=Tensor("BLSTM/Placeholder_1:0", shape=(?,), dtype=int32), dense_shape=Tensor("BLSTM/Placeholder:0", shape=(?,), dtype=int64))
Tensor("transpose:0", shape=(?, ?, 102), dtype=float32)
Tensor("BLSTM/Placeholder_3:0", shape=(?,), dtype=int32)
loading train data, please wait--------------------- end= 
get image:  544534
loading validation data, please wait--------------------- end= 
get image:  486
2018-04-27 04:36:19.489752: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 04:36:19.489810: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 04:36:19.489820: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-27 04:36:19.597993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-04-27 04:36:19.598359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:04.0
Total memory: 11.17GiB
Free memory: 11.09GiB
2018-04-27 04:36:19.598383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-04-27 04:36:19.598391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-04-27 04:36:19.598403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
restore is true
restore from the checkpoint../checkpoint/ocr-model-24000
=============================begin training=============================
2018-04-27 04:36:26.834486: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4565 get requests, put_count=3100 evicted_count=1000 eviction_rate=0.322581 and unsatisfied allocation rate=0.561884
2018-04-27 04:36:26.834590: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-04-27 04:36:37.274474: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 643 get requests, put_count=1658 evicted_count=1000 eviction_rate=0.603136 and unsatisfied allocation rate=0.00155521
2018-04-27 04:36:42.964203: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 650 get requests, put_count=1677 evicted_count=1000 eviction_rate=0.596303 and unsatisfied allocation rate=0.00153846
2018-04-27 04:36:49.987547: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5029 get requests, put_count=3988 evicted_count=1000 eviction_rate=0.250752 and unsatisfied allocation rate=0.414595
2018-04-27 04:36:49.987634: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 493 to 542
2018-04-27 04:36:58.454180: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4800 get requests, put_count=4279 evicted_count=1000 eviction_rate=0.233699 and unsatisfied allocation rate=0.335
2018-04-27 04:36:58.454318: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 958 to 1053
2018-04-27 04:37:12.695312: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10649 get requests, put_count=10801 evicted_count=1000 eviction_rate=0.092584 and unsatisfied allocation rate=0.0970983
2018-04-27 04:37:12.695434: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2049 to 2253
batch 99 : time 1.4234123229980469
save checkpoint 24100
batch 199 : time 1.4187510013580322
save checkpoint 24200
batch 299 : time 1.4143614768981934
save checkpoint 24300
batch 399 : time 1.4140467643737793
save checkpoint 24400
batch 499 : time 1.417604684829712
save checkpoint 24500
batch 599 : time 1.4135148525238037
save checkpoint 24600
batch 699 : time 1.4175350666046143
save checkpoint 24700
batch 799 : time 1.4206862449645996
save checkpoint 24800
batch 899 : time 1.4053428173065186
save checkpoint 24900
batch 999 : time 1.4180412292480469
save checkpoint 25000
seq   0: origin: [78, 85, 67, 76, 69, 73, 90, 69] decoded:[83, 65, 69, 79, 84]
seq   1: origin: [35, 79, 77, 75, 69] decoded:[66, 79, 77, 75, 69]
seq   2: origin: [44, 69, 76, 67, 89] decoded:[44, 69, 76, 67, 89]
seq   3: origin: [39, 34, 47, 39, 34, 47, 42] decoded:[38, 38, 38, 45, 48, 34, 38]
seq   4: origin: [85, 78, 69, 88, 84, 69, 82, 82, 73, 84, 79, 82, 73, 65, 76, 73, 84, 89] decoded:[85, 78, 69, 88, 84, 69, 82, 82, 73, 84, 79, 82, 73, 65, 76, 73, 84, 89]
seq   5: origin: [57, 58, 45, 48, 53, 51, 58, 34] decoded:[83, 79, 79, 65, 83]
seq   6: origin: [52, 45, 34, 54, 47, 36, 41, 56, 34, 58, 52] decoded:[52, 45, 34, 48, 45, 48, 51, 47, 38]
seq   7: origin: [54, 47, 36, 34, 45, 45, 48, 56] decoded:[54, 47, 36, 34, 45, 45, 48, 56]
seq   8: origin: [67, 79, 78, 86, 69, 78, 69, 68] decoded:[67, 79, 78, 86, 69, 78, 83, 65]
seq   9: origin: [52, 49, 45, 34, 47, 36, 41, 47, 48, 45, 48, 40, 42, 52, 53] decoded:[52, 49, 45, 34, 47, 36, 41, 47, 48, 45, 48, 40, 42, 52, 53]
seq  10: origin: [68, 73, 83, 67, 73, 80, 76, 69, 83] decoded:[68, 73, 83, 67, 73, 80, 76, 69, 83]
seq  11: origin: [56, 65, 76, 76, 66, 65, 67, 75] decoded:[56, 65, 76, 76, 66, 65, 67, 75]
seq  12: origin: [20, 22, 24, 22, 18, 18, 18, 20, 20] decoded:[20, 22, 24, 22, 18, 18, 18, 20, 20]
seq  13: origin: [72, 69, 77, 80, 72, 69, 82, 68, 83] decoded:[72, 69, 77, 80, 72, 69, 82, 68, 83]
seq  14: origin: [79, 82, 84, 72, 79, 80, 84, 73, 67, 83] decoded:[79, 82, 84, 72, 79, 80, 84, 73, 67, 83]
seq  15: origin: [80, 69, 78, 84, 65, 80, 76, 79, 73, 68, 89] decoded:[80, 69, 78, 84, 65, 80, 76, 79, 73, 68, 89]
seq  16: origin: [78, 79, 78, 70, 79, 82, 69, 73, 71, 78] decoded:[80, 82, 65, 84, 69, 84, 72, 69, 82]
seq  17: origin: [65, 77, 69, 84, 82, 79, 77, 69, 84, 69, 82] decoded:[65, 77, 69, 84, 82, 79, 77, 69, 84, 69, 82]
4/27 5:0:23  step===25000, Epoch 1/1000, accuracy = 0.663,avg_train_cost = 5.636, lastbatch_err = 0.160, time = 1441.569,lr=0.00777822

batch 1099 : time 1.416898250579834
save checkpoint 25100
batch 1199 : time 1.4287352561950684
save checkpoint 25200
batch 1299 : time 1.4188215732574463
save checkpoint 25300
batch 1399 : time 1.4200060367584229
save checkpoint 25400
batch 1499 : time 1.416562795639038
save checkpoint 25500
batch 1599 : time 1.4136252403259277
save checkpoint 25600
batch 1699 : time 1.4244799613952637
save checkpoint 25700
batch 1799 : time 1.4167635440826416
save checkpoint 25800
batch 1899 : time 1.4284448623657227
save checkpoint 25900
batch 1999 : time 1.4171061515808105
save checkpoint 26000
seq   0: origin: [78, 85, 67, 76, 69, 73, 90, 69] decoded:[83, 82, 65, 69, 84]
seq   1: origin: [35, 79, 77, 75, 69] decoded:[66, 79, 77, 75, 69]
seq   2: origin: [44, 69, 76, 67, 89] decoded:[44, 69, 76, 67, 89]
seq   3: origin: [39, 34, 47, 39, 34, 47, 42] decoded:[36, 38, 34, 53, 48, 47, 40]
seq   4: origin: [85, 78, 69, 88, 84, 69, 82, 82, 73, 84, 79, 82, 73, 65, 76, 73, 84, 89] decoded:[85, 78, 69, 88, 84, 69, 82, 82, 73, 84, 79, 82, 73, 65, 76, 73, 84, 89]
seq   5: origin: [57, 58, 45, 48, 53, 51, 58, 34] decoded:[83, 82, 85, 51, 52]
seq   6: origin: [52, 45, 34, 54, 47, 36, 41, 56, 34, 58, 52] decoded:[34, 45, 34, 51, 48, 53, 53, 34, 47, 58]
seq   7: origin: [54, 47, 36, 34, 45, 45, 48, 56] decoded:[54, 47, 36, 34, 45, 45, 48, 56]
seq   8: origin: [67, 79, 78, 86, 69, 78, 69, 68] decoded:[67, 79, 78, 86, 69, 78, 69, 65]
seq   9: origin: [52, 49, 45, 34, 47, 36, 41, 47, 48, 45, 48, 40, 42, 52, 53] decoded:[52, 49, 45, 34, 47, 36, 41, 47, 48, 45, 48, 40, 42, 52, 53]
seq  10: origin: [68, 73, 83, 67, 73, 80, 76, 69, 83] decoded:[68, 73, 83, 67, 73, 80, 76, 69, 83]
seq  11: origin: [56, 65, 76, 76, 66, 65, 67, 75] decoded:[56, 65, 76, 76, 66, 65, 67, 75]
seq  12: origin: [20, 22, 24, 22, 18, 18, 18, 20, 20] decoded:[20, 22, 24, 22, 18, 18, 18, 20, 20]
seq  13: origin: [72, 69, 77, 80, 72, 69, 82, 68, 83] decoded:[72, 69, 77, 80, 72, 69, 82, 68, 83]
seq  14: origin: [79, 82, 84, 72, 79, 80, 84, 73, 67, 83] decoded:[79, 82, 84, 72, 79, 80, 84, 73, 67, 83]
seq  15: origin: [80, 69, 78, 84, 65, 80, 76, 79, 73, 68, 89] decoded:[80, 69, 78, 84, 65, 80, 76, 79, 73, 68, 89]
seq  16: origin: [78, 79, 78, 70, 79, 82, 69, 73, 71, 78] decoded:[80, 82, 65, 84, 69, 84, 73, 69, 82]
seq  17: origin: [65, 77, 69, 84, 82, 79, 77, 69, 84, 69, 82] decoded:[65, 77, 69, 84, 82, 79, 77, 69, 84, 69, 82]
4/27 5:24:11  step===26000, Epoch 1/1000, accuracy = 0.671,avg_train_cost = 5.668, lastbatch_err = 0.156, time = 2869.599,lr=0.00770043

batch 2099 : time 1.4061663150787354
save checkpoint 26100
save checkpoint 26200
batch 99 : time 1.426095724105835
save checkpoint 26300
batch 199 : time 1.4298782348632812
save checkpoint 26400
batch 299 : time 1.4244599342346191
save checkpoint 26500
batch 399 : time 1.4273834228515625
